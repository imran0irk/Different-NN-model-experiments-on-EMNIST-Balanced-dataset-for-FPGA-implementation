{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19a59457-6f70-43e1-85f0-2e7a69777458",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bda884c-b5f3-4fe6-aeb7-b386ad185a8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import save_model, load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import custom_object_scope\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.regularizers import l1  #reg ##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c82f23ee-f837-4222-9d46-7e9bffdb6aad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from qkeras import *\n",
    "from qkeras import QActivation\n",
    "from qkeras import QDense\n",
    "from qkeras import QConv2D\n",
    "from qkeras import quantized_bits, quantized_relu\n",
    "from qkeras.utils import load_qmodel\n",
    "from qkeras.utils import print_model_sparsity\n",
    "from qkeras.utils import model_save_quantized_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a6c24931-6591-49fb-b01f-5ee226e9e321",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import prune\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import pruning_callbacks\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import pruning_schedule\n",
    "\n",
    "from tensorflow_model_optimization.sparsity.keras import prune_low_magnitude, strip_pruning \n",
    "from tensorflow_model_optimization.sparsity.keras import ConstantSparsity\n",
    "from tensorflow_model_optimization.sparsity.keras import PolynomialDecay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82d383b4-923a-4105-a8ef-e897162c810d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from qkeras.utils import QN...\n",
    "\n",
    "gradual_qnoise_callback_0 = QNoiseScheduler(\n",
    "      start=2, finish=4, freq_type=\"step\", exponent=3.0)\n",
    "\n",
    "gradual_qnoise_callback_1 = QNoiseScheduler(\n",
    "      start=2, finish=10, freq_type=\"step\", exponent=3.0)\n",
    "\n",
    "gradual_qnoise_callback_2 = QNoiseScheduler(\n",
    "      start=2, finish=10, freq_type=\"step\", exponent=2.0)\n",
    "\n",
    "gradual_qnoise_callback_3 = QNoiseScheduler(\n",
    "      start=6, finish=10, freq_type=\"step\", exponent=3.0)\n",
    "\n",
    "gradual_qnoise_callback_4 = QNoiseScheduler(\n",
    "      start=6, finish=20, freq_type=\"step\", exponent=3.0)\n",
    "\n",
    "gradual_qnoise_callback_5 = QNoiseScheduler(\n",
    "      start=0, finish=20, freq_type=\"step\", update_freq=2, exponent=3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0f87aa45-14af-4add-9d39-f815b6f7a1a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pruning_params = {\n",
    "    'pruning_schedule': PolynomialDecay(\n",
    "        initial_sparsity=0.0, \n",
    "        final_sparsity=0.5,\n",
    "        begin_step=1000, end_step=10000,\n",
    "        frequency=100\n",
    "    )\n",
    "}\n",
    "\n",
    "#pruning_params = {\n",
    "#    \"pruning_schedule\": ConstantSparsity(0.75, begin_step=2000, frequency=100)\n",
    "#    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8934cd0f-4694-4f07-a0af-012929d286a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NB_EPOCH = 15\n",
    "BATCH_SIZE = 64\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10\n",
    "VALIDATION_SPLIT = 0.1\n",
    "OPTIMIZER = Adam(learning_rate=0.001)\n",
    "#OPTIMIZER = SGD(learning_rate=0.001, momentum=0.9)\n",
    "\n",
    "W = 4  #QConv2d q_bit\n",
    "I = 0\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    return lr * tf.math.exp(-0.1)\n",
    "\n",
    "CALLBACKS = lr_schedule = LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d6b6c63-4145-40c0-85f6-26d668f64687",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model(input_shape):\n",
    "    x = x_in = Input(shape=input_shape, name=\"input_layer\")\n",
    "    a = QConv2D(\n",
    "        32, (2, 2), strides=(2, 2),\n",
    "        kernel_quantizer=quantized_bits(W, I, 1),\n",
    "        bias_quantizer=quantized_bits(W, I, 1),\n",
    "        kernel_regularizer=l1(1e-5),                                 #####Added L1\n",
    "        name=\"conv2d_L1\")(x)\n",
    "    b = QActivation(\"quantized_relu(4, 0)\", name=\"activation_1\")(a)\n",
    "    c = QConv2D(\n",
    "        64, (3, 3), strides=(2, 2),\n",
    "        kernel_quantizer=quantized_bits(W, I, 1),\n",
    "        bias_quantizer=quantized_bits(W, I, 1),\n",
    "        kernel_regularizer=l1(1e-5),\n",
    "        name=\"conv2d_L2\")(b)\n",
    "    d = QActivation(\"quantized_relu(4, 0)\", name=\"activation_2\")(c)\n",
    "    e = QConv2D(\n",
    "        64, (2, 2), strides=(2, 2),\n",
    "        kernel_quantizer=quantized_bits(W, I, 1),\n",
    "        bias_quantizer=quantized_bits(W, I, 1),\n",
    "        kernel_regularizer=l1(1e-5),\n",
    "        name=\"conv2d_L3\")(d)\n",
    "    f = QActivation(\"quantized_relu(4, 0)\", name=\"activation_3\")(e)\n",
    "    g = Flatten()(f)\n",
    "    h = QDense(NB_CLASSES, kernel_quantizer=quantized_bits(W, I, 1),\n",
    "                   bias_quantizer=quantized_bits(W, I, 1),\n",
    "                   name=\"dense\")(g)\n",
    "    i = Activation(\"softmax\", name=\"Softmax\")(h)\n",
    "\n",
    "    model = Model(inputs=[x_in], outputs=[i])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9ba848ce-7496-4ac9-94c0-e9961ed1fcef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_and_save(model, x_train, y_train, x_test, y_test):\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\",\n",
    "     #   loss=\"sparse_categorical_crossentropy\",\n",
    "     #   loss= keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer= OPTIMIZER,\n",
    "        metrics=[\"accuracy\"])\n",
    "    \n",
    "    # Print the model summary.\n",
    "    model.summary()\n",
    "    \n",
    "  #  callbacks=[\n",
    "   #     gradual_qnoise_callback_4\n",
    "    #]\n",
    "    CALLBACKS = [\n",
    "    tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "    tfmot.sparsity.keras.PruningSummaries(log_dir='pruning_logs')\n",
    "    ]\n",
    "    \n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=NB_EPOCH,\n",
    "        validation_split=VALIDATION_SPLIT,\n",
    "        verbose=VERBOSE,\n",
    "        callbacks=CALLBACKS,\n",
    "        validation_data=(x_test, y_test))\n",
    "    \n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(\"Test loss:\", score[0])\n",
    "    print(\"Test accuracy:\", score[1])\n",
    "\n",
    "    print_model_sparsity(model)\n",
    "    \n",
    "    # Export and import the model. Check that accuracy persists.\n",
    "    _, keras_file = tempfile.mkstemp(suffix=\".keras\")  #\n",
    "    print(\"Saving model to:\", keras_file)\n",
    "    model.save(\"modelx.keras\", save_format=\"keras\")\n",
    "    model.save_weights(\"modelx_w.keras\")\n",
    "    print(\"Model saved to --- .keras\")\n",
    "    print_qstats(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "291e9227-6b84-4608-8477-ff9b62b8a7e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = np.load('/home/mindspore/work/mnist.npz')\n",
    "x_train, y_train = data['x_train'], data['y_train']\n",
    "x_test, y_test = data['x_test'], data['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "868e5682-85ef-4ca1-8671-cafc1f571fd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "if K.image_data_format() == \"channels_first\":\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2954d003-1270-4d97-91ff-3e2436aa8c97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.astype(\"float32\")\n",
    "x_test = x_test.astype(\"float32\")\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "# convert class vectors to binary class matrices //one-hot key 0-9 class matrice lable 0/1\n",
    "y_train = to_categorical(y_train, NB_CLASSES)\n",
    "y_test = to_categorical(y_test, NB_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4950914a-0b7b-4114-9d01-5601be7ba0aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = build_model(input_shape)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0f19d7fe-1948-4544-890f-46d2319c4e0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = prune_low_magnitude(model, **pruning_params)   ##added from tfmot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a02c2a01-6c3b-4f3a-a121-ad1015ccc49d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_layer (InputLayer)    [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " prune_low_magnitude_Q_conv  (None, 14, 14, 32)        290       \n",
      " 2d_L1 (PruneLowMagnitude)                                       \n",
      "                                                                 \n",
      " prune_low_magnitude_Q_acti  (None, 14, 14, 32)        1         \n",
      " vation_1 (PruneLowMagnitud                                      \n",
      " e)                                                              \n",
      "                                                                 \n",
      " prune_low_magnitude_Q_conv  (None, 6, 6, 64)          36930     \n",
      " 2d_L2 (PruneLowMagnitude)                                       \n",
      "                                                                 \n",
      " prune_low_magnitude_Q_acti  (None, 6, 6, 64)          1         \n",
      " vation_2 (PruneLowMagnitud                                      \n",
      " e)                                                              \n",
      "                                                                 \n",
      " prune_low_magnitude_Q_conv  (None, 3, 3, 64)          32834     \n",
      " 2d_L3 (PruneLowMagnitude)                                       \n",
      "                                                                 \n",
      " prune_low_magnitude_Q_acti  (None, 3, 3, 64)          1         \n",
      " vation_3 (PruneLowMagnitud                                      \n",
      " e)                                                              \n",
      "                                                                 \n",
      " prune_low_magnitude_flatte  (None, 576)               1         \n",
      " n (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_dense   (None, 10)                11532     \n",
      " (PruneLowMagnitude)                                             \n",
      "                                                                 \n",
      " prune_low_magnitude_Softma  (None, 10)                1         \n",
      " x (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 81591 (318.75 KB)\n",
      "Trainable params: 40874 (159.66 KB)\n",
      "Non-trainable params: 40717 (159.09 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "  1/938 [..............................] - ETA: 1:42:03 - loss: 2.3284e-04 - accuracy: 1.0000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0083s vs `on_train_batch_end` time: 0.0153s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0083s vs `on_train_batch_end` time: 0.0153s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 18s 13ms/step - loss: 1.3933e-04 - accuracy: 1.0000 - val_loss: 0.0516 - val_accuracy: 0.9883\n",
      "Epoch 2/15\n",
      "938/938 [==============================] - 10s 11ms/step - loss: 1.0324e-04 - accuracy: 1.0000 - val_loss: 0.0524 - val_accuracy: 0.9874\n",
      "Epoch 3/15\n",
      "938/938 [==============================] - 10s 11ms/step - loss: 1.4355e-04 - accuracy: 1.0000 - val_loss: 0.0529 - val_accuracy: 0.9876\n",
      "Epoch 4/15\n",
      "938/938 [==============================] - 10s 11ms/step - loss: 4.3948e-04 - accuracy: 0.9999 - val_loss: 0.0548 - val_accuracy: 0.9867\n",
      "Epoch 5/15\n",
      "938/938 [==============================] - 10s 11ms/step - loss: 8.9106e-04 - accuracy: 0.9998 - val_loss: 0.0559 - val_accuracy: 0.9867\n",
      "Epoch 6/15\n",
      "938/938 [==============================] - 10s 11ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.0584 - val_accuracy: 0.9850\n",
      "Epoch 7/15\n",
      "938/938 [==============================] - 10s 11ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.0545 - val_accuracy: 0.9860\n",
      "Epoch 8/15\n",
      "938/938 [==============================] - 11s 11ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.0591 - val_accuracy: 0.9851\n",
      "Epoch 9/15\n",
      "938/938 [==============================] - 11s 11ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 0.0571 - val_accuracy: 0.9866\n",
      "Epoch 10/15\n",
      "938/938 [==============================] - 11s 11ms/step - loss: 9.4179e-04 - accuracy: 1.0000 - val_loss: 0.0561 - val_accuracy: 0.9868\n",
      "Epoch 11/15\n",
      "938/938 [==============================] - 10s 11ms/step - loss: 6.0942e-04 - accuracy: 1.0000 - val_loss: 0.0564 - val_accuracy: 0.9863\n",
      "Epoch 12/15\n",
      "938/938 [==============================] - 11s 11ms/step - loss: 5.8075e-04 - accuracy: 1.0000 - val_loss: 0.0570 - val_accuracy: 0.9864\n",
      "Epoch 13/15\n",
      "938/938 [==============================] - 11s 11ms/step - loss: 5.6067e-04 - accuracy: 1.0000 - val_loss: 0.0558 - val_accuracy: 0.9860\n",
      "Epoch 14/15\n",
      "938/938 [==============================] - 11s 11ms/step - loss: 5.0681e-04 - accuracy: 1.0000 - val_loss: 0.0556 - val_accuracy: 0.9866\n",
      "Epoch 15/15\n",
      "938/938 [==============================] - 11s 11ms/step - loss: 4.5413e-04 - accuracy: 1.0000 - val_loss: 0.0562 - val_accuracy: 0.9865\n",
      "Test loss: 0.056193284690380096\n",
      "Test accuracy: 0.9865000247955322\n",
      "Model Sparsity Summary (model)\n",
      "--\n",
      "prune_low_magnitude_Q_conv2d_L1: (Q_conv2d_L1/kernel:0, 0.5)\n",
      "prune_low_magnitude_Q_conv2d_L2: (Q_conv2d_L2/kernel:0, 0.5)\n",
      "prune_low_magnitude_Q_conv2d_L3: (Q_conv2d_L3/kernel:0, 0.5)\n",
      "prune_low_magnitude_dense: (dense/kernel:0, 0.5)\n",
      "\n",
      "\n",
      "Saving model to: /tmp/tmpzawbyxlx.keras\n",
      "Model saved to --- .keras\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Please initialize `Prune` layer with a `Layer` instance. You passed: QConv2D",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#compile_ and_ train model and save data\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrain_and_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[42], line 42\u001b[0m, in \u001b[0;36mtrain_and_save\u001b[0;34m(model, x_train, y_train, x_test, y_test)\u001b[0m\n\u001b[1;32m     40\u001b[0m model\u001b[38;5;241m.\u001b[39msave_weights(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodelx_w.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved to --- .keras\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m \u001b[43mprint_qstats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/jupyter/lib/python3.9/site-packages/qkeras/estimate.py:616\u001b[0m, in \u001b[0;36mprint_qstats\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_qstats\u001b[39m(model):\n\u001b[1;32m    614\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Prints quantization statistics for the model.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 616\u001b[0m   model_ops \u001b[38;5;241m=\u001b[39m \u001b[43mextract_model_operations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    618\u001b[0m   ops_table \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    620\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda/envs/jupyter/lib/python3.9/site-packages/qkeras/estimate.py:376\u001b[0m, in \u001b[0;36mextract_model_operations\u001b[0;34m(in_model)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_model_operations\u001b[39m(in_model):\n\u001b[1;32m    374\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Determines types of operations for convolutions.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 376\u001b[0m   model \u001b[38;5;241m=\u001b[39m \u001b[43munfold_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m   cache_q \u001b[38;5;241m=\u001b[39m create_activation_cache(model)\n\u001b[1;32m    378\u001b[0m   cache_o \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/miniconda/envs/jupyter/lib/python3.9/site-packages/qkeras/bn_folding_utils.py:133\u001b[0m, in \u001b[0;36munfold_model\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    130\u001b[0m     new_layer\u001b[38;5;241m.\u001b[39mset_weights(src_layer\u001b[38;5;241m.\u001b[39mget_weights())\n\u001b[1;32m    132\u001b[0m inp \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39minput_shape[\u001b[38;5;241m1\u001b[39m:])\n\u001b[0;32m--> 133\u001b[0m cloned_model \u001b[38;5;241m=\u001b[39m \u001b[43mclone_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclone_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_convert_folded_layer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# replace weights\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (src_layer, new_layer) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(model\u001b[38;5;241m.\u001b[39mlayers, cloned_model\u001b[38;5;241m.\u001b[39mlayers):\n",
      "File \u001b[0;32m~/miniconda/envs/jupyter/lib/python3.9/site-packages/keras/src/models/cloning.py:539\u001b[0m, in \u001b[0;36mclone_model\u001b[0;34m(model, input_tensors, clone_function)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, functional\u001b[38;5;241m.\u001b[39mFunctional):\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;66;03m# If the get_config() method is the same as a regular Functional\u001b[39;00m\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;66;03m# model, we're safe to use _clone_functional_model (which relies\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;66;03m# or input_tensors are passed, we attempt it anyway\u001b[39;00m\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;66;03m# in order to preserve backwards compatibility.\u001b[39;00m\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generic_utils\u001b[38;5;241m.\u001b[39mis_default(model\u001b[38;5;241m.\u001b[39mget_config) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    537\u001b[0m         clone_function \u001b[38;5;129;01mor\u001b[39;00m input_tensors\n\u001b[1;32m    538\u001b[0m     ):\n\u001b[0;32m--> 539\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_clone_functional_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclone_function\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;66;03m# Case of a custom model class\u001b[39;00m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clone_function \u001b[38;5;129;01mor\u001b[39;00m input_tensors:\n",
      "File \u001b[0;32m~/miniconda/envs/jupyter/lib/python3.9/site-packages/keras/src/models/cloning.py:222\u001b[0m, in \u001b[0;36m_clone_functional_model\u001b[0;34m(model, input_tensors, layer_fn)\u001b[0m\n\u001b[1;32m    218\u001b[0m         model_configs, created_layers \u001b[38;5;241m=\u001b[39m _clone_layers_and_model_config(\n\u001b[1;32m    219\u001b[0m             model, new_input_layers, layer_fn\n\u001b[1;32m    220\u001b[0m         )\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 222\u001b[0m     model_configs, created_layers \u001b[38;5;241m=\u001b[39m \u001b[43m_clone_layers_and_model_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_input_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_fn\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# Reconstruct model from the config, using the cloned layers.\u001b[39;00m\n\u001b[1;32m    226\u001b[0m (\n\u001b[1;32m    227\u001b[0m     input_tensors,\n\u001b[1;32m    228\u001b[0m     output_tensors,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    231\u001b[0m     model_configs, created_layers\u001b[38;5;241m=\u001b[39mcreated_layers\n\u001b[1;32m    232\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda/envs/jupyter/lib/python3.9/site-packages/keras/src/models/cloning.py:298\u001b[0m, in \u001b[0;36m_clone_layers_and_model_config\u001b[0;34m(model, input_layers, layer_fn)\u001b[0m\n\u001b[1;32m    295\u001b[0m         created_layers[layer\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m layer_fn(layer)\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n\u001b[0;32m--> 298\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_network_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserialize_layer_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_copy_layer\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m config, created_layers\n",
      "File \u001b[0;32m~/miniconda/envs/jupyter/lib/python3.9/site-packages/keras/src/engine/functional.py:1590\u001b[0m, in \u001b[0;36mget_network_config\u001b[0;34m(network, serialize_layer_fn, config)\u001b[0m\n\u001b[1;32m   1588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, Functional) \u001b[38;5;129;01mand\u001b[39;00m set_layers_legacy:\n\u001b[1;32m   1589\u001b[0m     layer\u001b[38;5;241m.\u001b[39muse_legacy_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 1590\u001b[0m layer_config \u001b[38;5;241m=\u001b[39m \u001b[43mserialize_layer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1591\u001b[0m layer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m   1592\u001b[0m layer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minbound_nodes\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m filtered_inbound_nodes\n",
      "File \u001b[0;32m~/miniconda/envs/jupyter/lib/python3.9/site-packages/keras/src/models/cloning.py:295\u001b[0m, in \u001b[0;36m_clone_layers_and_model_config.<locals>._copy_layer\u001b[0;34m(layer)\u001b[0m\n\u001b[1;32m    293\u001b[0m     created_layers[layer\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m InputLayer(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlayer\u001b[38;5;241m.\u001b[39mget_config())\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 295\u001b[0m     created_layers[layer\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {}\n",
      "File \u001b[0;32m~/miniconda/envs/jupyter/lib/python3.9/site-packages/qkeras/bn_folding_utils.py:105\u001b[0m, in \u001b[0;36munfold_model.<locals>._convert_folded_layer\u001b[0;34m(layer)\u001b[0m\n\u001b[1;32m    103\u001b[0m   new_layer \u001b[38;5;241m=\u001b[39m convert_folded_layer_to_unfolded(layer)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m   new_layer \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m new_layer\u001b[38;5;241m.\u001b[39mbuild(layer\u001b[38;5;241m.\u001b[39minput_shape)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_layer\n",
      "File \u001b[0;32m~/miniconda/envs/jupyter/lib/python3.9/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py:361\u001b[0m, in \u001b[0;36mPruneLowMagnitude.from_config\u001b[0;34m(cls, config)\u001b[0m\n\u001b[1;32m    358\u001b[0m layer \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mdeserialize(config\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    359\u001b[0m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m layer\n\u001b[0;32m--> 361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/jupyter/lib/python3.9/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py:171\u001b[0m, in \u001b[0;36mPruneLowMagnitude.__init__\u001b[0;34m(self, layer, pruning_schedule, block_size, block_pooling_type, sparsity_m_by_n, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    167\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnsupported pooling type \u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m. Should be \u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mAVG\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m or \u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mMAX\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    168\u001b[0m       \u001b[38;5;241m.\u001b[39mformat(block_pooling_type))\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mLayer):\n\u001b[0;32m--> 171\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    172\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlease initialize `Prune` layer with a \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    173\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`Layer` instance. You passed: \u001b[39m\u001b[38;5;132;01m{input}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mlayer))\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# TODO(pulkitb): This should be pushed up to the wrappers.py\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# Name the layer using the wrapper and underlying layer name.\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# Prune(Dense) becomes prune_dense_1\u001b[39;00m\n\u001b[1;32m    178\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m    179\u001b[0m     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_to_snake_case(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m    180\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Please initialize `Prune` layer with a `Layer` instance. You passed: QConv2D"
     ]
    }
   ],
   "source": [
    "\n",
    "#compile_ and_ train model and save data\n",
    "\n",
    "train_and_save(model, x_train, y_train, x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "609a98bb-e5d2-4921-bab8-910006259426",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading model\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_layer (InputLayer)    [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " prune_low_magnitude_Q_conv  (None, 14, 14, 32)        290       \n",
      " 2d_L1 (PruneLowMagnitude)                                       \n",
      "                                                                 \n",
      " prune_low_magnitude_Q_acti  (None, 14, 14, 32)        1         \n",
      " vation_1 (PruneLowMagnitud                                      \n",
      " e)                                                              \n",
      "                                                                 \n",
      " prune_low_magnitude_Q_conv  (None, 6, 6, 64)          36930     \n",
      " 2d_L2 (PruneLowMagnitude)                                       \n",
      "                                                                 \n",
      " prune_low_magnitude_Q_acti  (None, 6, 6, 64)          1         \n",
      " vation_2 (PruneLowMagnitud                                      \n",
      " e)                                                              \n",
      "                                                                 \n",
      " prune_low_magnitude_Q_conv  (None, 3, 3, 64)          32834     \n",
      " 2d_L3 (PruneLowMagnitude)                                       \n",
      "                                                                 \n",
      " prune_low_magnitude_Q_acti  (None, 3, 3, 64)          1         \n",
      " vation_3 (PruneLowMagnitud                                      \n",
      " e)                                                              \n",
      "                                                                 \n",
      " prune_low_magnitude_flatte  (None, 576)               1         \n",
      " n (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_dense   (None, 10)                11532     \n",
      " (PruneLowMagnitude)                                             \n",
      "                                                                 \n",
      " prune_low_magnitude_Softma  (None, 10)                1         \n",
      " x (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 81591 (318.75 KB)\n",
      "Trainable params: 40874 (159.66 KB)\n",
      "Non-trainable params: 40717 (159.09 KB)\n",
      "_________________________________________________________________\n",
      "Test loss: 0.056193284690380096\n",
      "Test accuracy: 0.9865000247955322\n",
      "Model type -  <class 'keras.src.engine.functional.Functional'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Reloading model\")\n",
    "    \n",
    "#with custom_object_scope({'TFOpLambda': Lambda}):\n",
    "   #     loaded_model = load_qmodel('modelx.keras')\n",
    "      #  loaded_model.model_save_quantized_weights(\"modelx1.keras\")\n",
    "      #  loaded_model1 = load_qmodel(\"modelx1.keras\")\n",
    "        \n",
    "with prune.prune_scope():\n",
    "    loaded_model = load_qmodel('modelx.keras')\n",
    "\n",
    "loaded_model.summary()\n",
    "\n",
    "#loaded_model1.summary()\n",
    "\n",
    "\n",
    "score = loaded_model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])\n",
    "\n",
    "print(\"Model type - \",type(loaded_model))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0d851150-c91a-445a-8cae-e7325d935eb4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Please initialize `Prune` layer with a `Layer` instance. You passed: QConv2D",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprint_qstats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloaded_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type - \u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28mtype\u001b[39m(loaded_model))\n",
      "File \u001b[0;32m~/miniconda/envs/jupyter/lib/python3.9/site-packages/qkeras/estimate.py:616\u001b[0m, in \u001b[0;36mprint_qstats\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_qstats\u001b[39m(model):\n\u001b[1;32m    614\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Prints quantization statistics for the model.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 616\u001b[0m   model_ops \u001b[38;5;241m=\u001b[39m \u001b[43mextract_model_operations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    618\u001b[0m   ops_table \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    620\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda/envs/jupyter/lib/python3.9/site-packages/qkeras/estimate.py:376\u001b[0m, in \u001b[0;36mextract_model_operations\u001b[0;34m(in_model)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_model_operations\u001b[39m(in_model):\n\u001b[1;32m    374\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Determines types of operations for convolutions.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 376\u001b[0m   model \u001b[38;5;241m=\u001b[39m \u001b[43munfold_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m   cache_q \u001b[38;5;241m=\u001b[39m create_activation_cache(model)\n\u001b[1;32m    378\u001b[0m   cache_o \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/miniconda/envs/jupyter/lib/python3.9/site-packages/qkeras/bn_folding_utils.py:133\u001b[0m, in \u001b[0;36munfold_model\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    130\u001b[0m     new_layer\u001b[38;5;241m.\u001b[39mset_weights(src_layer\u001b[38;5;241m.\u001b[39mget_weights())\n\u001b[1;32m    132\u001b[0m inp \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39minput_shape[\u001b[38;5;241m1\u001b[39m:])\n\u001b[0;32m--> 133\u001b[0m cloned_model \u001b[38;5;241m=\u001b[39m \u001b[43mclone_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclone_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_convert_folded_layer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# replace weights\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (src_layer, new_layer) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(model\u001b[38;5;241m.\u001b[39mlayers, cloned_model\u001b[38;5;241m.\u001b[39mlayers):\n",
      "File \u001b[0;32m~/miniconda/envs/jupyter/lib/python3.9/site-packages/keras/src/models/cloning.py:539\u001b[0m, in \u001b[0;36mclone_model\u001b[0;34m(model, input_tensors, clone_function)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, functional\u001b[38;5;241m.\u001b[39mFunctional):\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;66;03m# If the get_config() method is the same as a regular Functional\u001b[39;00m\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;66;03m# model, we're safe to use _clone_functional_model (which relies\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;66;03m# or input_tensors are passed, we attempt it anyway\u001b[39;00m\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;66;03m# in order to preserve backwards compatibility.\u001b[39;00m\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generic_utils\u001b[38;5;241m.\u001b[39mis_default(model\u001b[38;5;241m.\u001b[39mget_config) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    537\u001b[0m         clone_function \u001b[38;5;129;01mor\u001b[39;00m input_tensors\n\u001b[1;32m    538\u001b[0m     ):\n\u001b[0;32m--> 539\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_clone_functional_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclone_function\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;66;03m# Case of a custom model class\u001b[39;00m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clone_function \u001b[38;5;129;01mor\u001b[39;00m input_tensors:\n",
      "File \u001b[0;32m~/miniconda/envs/jupyter/lib/python3.9/site-packages/keras/src/models/cloning.py:222\u001b[0m, in \u001b[0;36m_clone_functional_model\u001b[0;34m(model, input_tensors, layer_fn)\u001b[0m\n\u001b[1;32m    218\u001b[0m         model_configs, created_layers \u001b[38;5;241m=\u001b[39m _clone_layers_and_model_config(\n\u001b[1;32m    219\u001b[0m             model, new_input_layers, layer_fn\n\u001b[1;32m    220\u001b[0m         )\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 222\u001b[0m     model_configs, created_layers \u001b[38;5;241m=\u001b[39m \u001b[43m_clone_layers_and_model_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_input_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_fn\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# Reconstruct model from the config, using the cloned layers.\u001b[39;00m\n\u001b[1;32m    226\u001b[0m (\n\u001b[1;32m    227\u001b[0m     input_tensors,\n\u001b[1;32m    228\u001b[0m     output_tensors,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    231\u001b[0m     model_configs, created_layers\u001b[38;5;241m=\u001b[39mcreated_layers\n\u001b[1;32m    232\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda/envs/jupyter/lib/python3.9/site-packages/keras/src/models/cloning.py:298\u001b[0m, in \u001b[0;36m_clone_layers_and_model_config\u001b[0;34m(model, input_layers, layer_fn)\u001b[0m\n\u001b[1;32m    295\u001b[0m         created_layers[layer\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m layer_fn(layer)\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n\u001b[0;32m--> 298\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_network_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserialize_layer_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_copy_layer\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m config, created_layers\n",
      "File \u001b[0;32m~/miniconda/envs/jupyter/lib/python3.9/site-packages/keras/src/engine/functional.py:1590\u001b[0m, in \u001b[0;36mget_network_config\u001b[0;34m(network, serialize_layer_fn, config)\u001b[0m\n\u001b[1;32m   1588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, Functional) \u001b[38;5;129;01mand\u001b[39;00m set_layers_legacy:\n\u001b[1;32m   1589\u001b[0m     layer\u001b[38;5;241m.\u001b[39muse_legacy_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 1590\u001b[0m layer_config \u001b[38;5;241m=\u001b[39m \u001b[43mserialize_layer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1591\u001b[0m layer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m   1592\u001b[0m layer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minbound_nodes\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m filtered_inbound_nodes\n",
      "File \u001b[0;32m~/miniconda/envs/jupyter/lib/python3.9/site-packages/keras/src/models/cloning.py:295\u001b[0m, in \u001b[0;36m_clone_layers_and_model_config.<locals>._copy_layer\u001b[0;34m(layer)\u001b[0m\n\u001b[1;32m    293\u001b[0m     created_layers[layer\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m InputLayer(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlayer\u001b[38;5;241m.\u001b[39mget_config())\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 295\u001b[0m     created_layers[layer\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {}\n",
      "File \u001b[0;32m~/miniconda/envs/jupyter/lib/python3.9/site-packages/qkeras/bn_folding_utils.py:105\u001b[0m, in \u001b[0;36munfold_model.<locals>._convert_folded_layer\u001b[0;34m(layer)\u001b[0m\n\u001b[1;32m    103\u001b[0m   new_layer \u001b[38;5;241m=\u001b[39m convert_folded_layer_to_unfolded(layer)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m   new_layer \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m new_layer\u001b[38;5;241m.\u001b[39mbuild(layer\u001b[38;5;241m.\u001b[39minput_shape)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_layer\n",
      "File \u001b[0;32m~/miniconda/envs/jupyter/lib/python3.9/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py:361\u001b[0m, in \u001b[0;36mPruneLowMagnitude.from_config\u001b[0;34m(cls, config)\u001b[0m\n\u001b[1;32m    358\u001b[0m layer \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mdeserialize(config\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    359\u001b[0m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m layer\n\u001b[0;32m--> 361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/jupyter/lib/python3.9/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py:171\u001b[0m, in \u001b[0;36mPruneLowMagnitude.__init__\u001b[0;34m(self, layer, pruning_schedule, block_size, block_pooling_type, sparsity_m_by_n, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    167\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnsupported pooling type \u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m. Should be \u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mAVG\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m or \u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mMAX\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    168\u001b[0m       \u001b[38;5;241m.\u001b[39mformat(block_pooling_type))\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mLayer):\n\u001b[0;32m--> 171\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    172\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlease initialize `Prune` layer with a \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    173\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`Layer` instance. You passed: \u001b[39m\u001b[38;5;132;01m{input}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mlayer))\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# TODO(pulkitb): This should be pushed up to the wrappers.py\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# Name the layer using the wrapper and underlying layer name.\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# Prune(Dense) becomes prune_dense_1\u001b[39;00m\n\u001b[1;32m    178\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m    179\u001b[0m     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_to_snake_case(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m    180\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Please initialize `Prune` layer with a `Layer` instance. You passed: QConv2D"
     ]
    }
   ],
   "source": [
    "print_qstats(loaded_model)\n",
    "print(\"Model type - \",type(loaded_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fb92094f-8480-49cc-964f-c4e42b4fa7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#After training, use tfmot.sparsity.keras.strip_pruning(model) to remove pruning wrappers, leaving a sparse model ready for deployment.\n",
    "\n",
    "model3 = tfmot.sparsity.keras.strip_pruning(loaded_model)  #3x lighter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d02f8399-e89c-4d52-899e-10789e70f7b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(\n",
    "    initial_sparsity=0.0,\n",
    "    final_sparsity=0.5,\n",
    "    begin_step=0,\n",
    "    end_step=2000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4603a60d-1ebe-4f2d-a701-73ccedef3acf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "score = model3.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])\n",
    "\n",
    "print(\"Model type - \",type(model3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c3285c89-a5b8-4f97-813f-def509d6db60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model3.save(\"pruned_final_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "431d95ca-7b57-4ec4-87aa-0115b9266ae1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_layer (InputLayer)    [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " Q_conv2d_L1 (QConv2D)       (None, 14, 14, 32)        160       \n",
      "                                                                 \n",
      " Q_activation_1 (QActivatio  (None, 14, 14, 32)        0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " Q_conv2d_L2 (QConv2D)       (None, 6, 6, 64)          18496     \n",
      "                                                                 \n",
      " Q_activation_2 (QActivatio  (None, 6, 6, 64)          0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " Q_conv2d_L3 (QConv2D)       (None, 3, 3, 64)          16448     \n",
      "                                                                 \n",
      " Q_activation_3 (QActivatio  (None, 3, 3, 64)          0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 576)               0         \n",
      "                                                                 \n",
      " dense (QDense)              (None, 10)                5770      \n",
      "                                                                 \n",
      " Softmax (Activation)        (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40874 (159.66 KB)\n",
      "Trainable params: 40874 (159.66 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f91e60-b2f6-46c8-913a-86179ab5a248",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
